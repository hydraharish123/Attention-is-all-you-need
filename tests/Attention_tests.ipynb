{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81aafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b56d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim, attention_dim):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = nn.Linear(in_features=encoder_hidden_dim, out_features=attention_dim)\n",
    "        self.W2 = nn.Linear(in_features=decoder_hidden_dim, out_features=decoder_hidden_dim)\n",
    "        self.V = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        '''\n",
    "            encoder_outputs ----> shape (batch, input_len, encoder_hidden_dim)\n",
    "            decoder_hidden  ----> shape (batch, decoder_hidden_dim)\n",
    "        '''\n",
    "\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1) # decoder_hidden_shape ---> (batch, 1, decoder_hidden_dim)\n",
    "\n",
    "        score = F.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden))\n",
    "        # score shape ---> (batch, input_len, attention_dim)\n",
    "\n",
    "        energy = self.V(score).squeeze(-1)\n",
    "        # energy shape ----> (batch, input_len)\n",
    "\n",
    "        attention_weighs = F.softmax(energy, dim=1)\n",
    "        # attention_weights ----> (batch, input_len)\n",
    "\n",
    "        # attention_weights.unsqueeze(1) ---> (batch, 1, input_len)\n",
    "        # encoder_inputs                 ---> (batch, input_len, encoder_hidden)\n",
    "        context = torch.bmm(attention_weighs.unsqueeze(1), encoder_outputs) # shape --> (batch, 1, encoder_hidden_dim)\n",
    "\n",
    "        context_vector = context.squeeze(1) # shape ---> (batch, encoder_hidden_dim)\n",
    "\n",
    "        return context_vector, attention_weighs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongDotAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LuongDotAttention, self).__init__()\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        '''\n",
    "            encoder_outputs ----> shape (batch, input_len, encoder_hidden_dim)\n",
    "            decoder_hidden  ----> shape (batch, decoder_hidden_dim)\n",
    "        '''\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1) # shape ---> (batch, 1, decoder_hidden_dim)\n",
    "\n",
    "        scores = torch.bmm(decoder_hidden, encoder_outputs.transpose(1, 2))\n",
    "        # scores shape ----> (batch, 1, input_len)\n",
    "\n",
    "        attention_weighs = F.softmax(scores, dim=-1)\n",
    "        # scores shape ----> (batch, 1, input_len)\n",
    "\n",
    "        context = torch.bmm(attention_weighs, encoder_outputs) # (batch, 1, input_len) * (batch, input_len, encoder_hidden_dim)\n",
    "        # context shape ----> (batch, 1, encoder_hidden_dim)\n",
    "\n",
    "        context_vector = context.squeeze(1) # (batch, encoder_hidden_dim)\n",
    "        attention_weighs = attention_weighs.squeeze(1) # shape ---> (batch, input_len)\n",
    "\n",
    "        return context_vector, attention_weighs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongGeneralAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(LuongGeneralAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        '''\n",
    "            encoder_outputs ----> shape (batch, input_len, hidden_dim)\n",
    "            decoder_hidden  ----> shape (batch, hidden_dim)\n",
    "        '''\n",
    "\n",
    "        encoder_outputs = self.Wa(encoder_outputs)\n",
    "        # shape --> (batch, input_len, hidden_dim)\n",
    "\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1) # shape --> (batch, 1, hidden_dim)\n",
    "\n",
    "        scores = torch.bmm(decoder_hidden, encoder_outputs.transpose(1,2))\n",
    "        # shape --> (batch, 1, input_len)\n",
    "\n",
    "        attention_weights = F.softmax(scores.squeeze(1), dim=1)\n",
    "        # shape --> (batch, input_len)\n",
    "\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        # shape --> (batch, 1, hidden_dim)\n",
    "        \n",
    "        return context.squeeze(1), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40de2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongConcatAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, attention_dim):\n",
    "        super(LuongConcatAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_dim*2, attention_dim) ## for concatentation --> [hi ; st]\n",
    "        self.Va = nn.Linear(attention_dim,1)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        '''\n",
    "            encoder_outputs ----> shape (batch, input_len, hidden_dim)\n",
    "            decoder_hidden  ----> shape (batch, hidden_dim)\n",
    "        '''\n",
    "\n",
    "        _, input_len, _ = encoder_outputs.size()\n",
    "\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, input_len, 1)\n",
    "        # shape ---> (batch, hidden_dim) to (batch, input_len, hidden_dim)\n",
    "\n",
    "        concatenate = torch.cat((encoder_outputs, decoder_hidden), dim=2)\n",
    "        # shape --> (batch, input_len, hidden_dim * 2)\n",
    "\n",
    "        energy = F.tanh(self.Wa(concatenate)) # shape --> (batch, input_len, attention_dim)\n",
    "\n",
    "        scores = self.Va(energy).squeeze(2) # shape --> (batch, input_len)\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=1) \n",
    "        # shape --> (batch, input_len)\n",
    "\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        # shape --> (batch, 1, hidden_dim)\n",
    "\n",
    "        context_vector = context.squeeze(1) # shape --> (batch, hidden_dim)\n",
    "        return context_vector, attention_weights "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
